# Изучение и сравнение различных программ LLM

[![Изучение и сравнение различных программ LLM](./images/02-lesson-banner.png?WT.mc_id=academic-105485-koreyst)](https://learn.microsoft.com/_themes/docs.theme/master/en-us/_themes/global/video-embed.html?id=39aa0f98-826a-4f71-a24d-e888a8e80246?WT.mc_id=academic-105485-koreyst)

> *Нажмите на картинку выше, чтобы посмотреть видео этого урока*

В предыдущем уроке мы увидели, как генеративный ИИ меняет технологический ландшафт, как работают большие языковые модели (LLM) и как бизнес - например, наш стартап - может применять их в своих кейсах и развиваться! В этой главе мы хотим сравнить и сопоставить различные типы больших языковых моделей, LLM, чтобы понять их преимущества и недостатки.

Следующий шаг на пути нашего стартапа - изучение современного ландшафта больших языковых моделей (LLM) и понимание того, какие из них подходят для нашего случая использования.

## Introduction

В этом уроке мы рассмотрим:

- Различные типы LLM в современном ландшафте.
- Тестирование, итерации и сравнение различных моделей для вашего сценария использования в Azure.
- How to deploy an LLM.

## Цели обучения

После завершения этого урока вы сможете:

- Выберите подходящую модель для вашего случая.
- Поймите, как тестировать, проводить итерации и улучшать производительность вашей модели.
- Знайте, как предприятия внедряют модели.

## Разберитесь в различных типах LLM

Большие языковые модели (LLM) могут иметь несколько категорий, основанных на их архитектуре, обучающих данных и сценарии использования. Понимание этих различий поможет нашему стартапу выбрать подходящую модель для конкретного сценария, а также понять, как тестировать, итерировать и улучшать производительность.

Существует множество различных типов моделей LLM, выбор модели зависит от того, для чего вы собираетесь их использовать, от ваших данных, от того, сколько вы готовы заплатить, и от многого другого.

В зависимости от того, планируете ли вы использовать модели для работы с текстом, аудио, видео, изображениями и т. д., вы можете выбрать другой тип модели.

- **Распознавание аудио и речи**. Для этой цели отлично подойдут модели типа Whisper, поскольку они являются универсальными и нацелены на распознавание речи. Они обучаются на различных аудиозаписях и могут выполнять многоязычное распознавание речи. Подробнее о [модели типа Whisper здесь](https://platform.openai.com/docs/models/whisper?WT.mc_id=academic-105485-koreyst).

- **Создание изображений**. Для генерации изображений можно использовать DALL-E и Midjourney - два очень известных варианта. DALL-E предлагается компанией Azure OpenAI. [Подробнее о фильме "ДАЛЛ-И" читайте здесь](https://platform.openai.com/docs/models/dall-e?WT.mc_id=academic-105485-koreyst) а также в главе 9 этой учебной программы.

- **Генерация текста**. Большинство моделей обучаются генерации текста, и у вас есть большой выбор от GPT-3.5 до GPT-4. Они имеют разную стоимость, причем GPT-4 - самая дорогая. Стоит обратить внимание на [Azure Open AI playground](https://oai.azure.com/portal/playground?WT.mc_id=academic-105485-koreyst) чтобы оценить, какие модели лучше всего соответствуют вашим потребностям с точки зрения возможностей и стоимости.

Выбор модели означает, что вы получаете некоторые базовые возможности, однако этого может быть недостаточно. Часто у вас есть специфические данные компании, о которых вам нужно как-то сообщить LLM. Есть несколько вариантов, как к этому подступиться, подробнее об этом в следующих разделах.

### Модели Foundation в сравнении с LLM

Термин "Foundation Model" был [придуман исследователями из Стэнфорда](https://arxiv.org/abs/2108.07258?WT.mc_id=academic-105485-koreyst) и определяется как модель искусственного интеллекта, которая соответствует некоторым критериям, таким как:

- **Они обучаются с помощью неконтролируемого обучения или самоконтролируемого обучения**, что означает, что они обучаются на немаркированных мультимодальных данных и не требуют человеческой аннотации или маркировки данных для процесса обучения.
- **Это очень большие модели**, на основе очень глубоких нейронных сетей, обученных на миллиардах параметров.
- **Обычно они служат "фундаментом" для других моделей.**, Это означает, что их можно использовать как отправную точку для построения других моделей, на основе которых можно производить тонкую настройку.

![Foundation Models versus LLMs](./images/FoundationModel.png?WT.mc_id=academic-105485-koreyst)

Image source: [Essential Guide to Foundation Models and Large Language Models | by Babar M Bhatti | Medium
](https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404)

Чтобы прояснить это различие, давайте рассмотрим ChatGPT на примере. Для создания первой версии ChatGPT в качестве базовой модели использовалась модель под названием GPT-3.5. Это означает, что OpenAI использовал некоторые данные, специфичные для чатов, чтобы создать настроенную версию GPT-3.5, которая была специализирована для работы в разговорных сценариях, таких как чат-боты.

![Foundation Model](./images/Multimodal.png?WT.mc_id=academic-105485-koreyst)

Image source: [2108.07258.pdf (arxiv.org)](https://arxiv.org/pdf/2108.07258.pdf?WT.mc_id=academic-105485-koreyst)

### Модели с открытым исходным кодом и проприетарные модели

Еще один способ классифицировать LLM - открытые или проприетарные.

Модели с открытым исходным кодом - это модели, которые находятся в открытом доступе и могут использоваться всеми желающими. Часто они предоставляются компанией, которая их создала, или исследовательским сообществом. Эти модели можно проверять, изменять и настраивать для различных случаев использования в LLM. Однако они не всегда оптимизированы для использования в производстве и могут быть не столь производительны, как проприетарные модели. Кроме того, финансирование моделей с открытым исходным кодом может быть ограничено, и они могут не поддерживаться в течение длительного времени или не обновляться с учетом последних исследований. Примеры популярных моделей с открытым исходным кодом включают [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html?WT.mc_id=academic-105485-koreyst), [Bloom](https://sapling.ai/llm/bloom?WT.mc_id=academic-105485-koreyst) и [LLaMA](https://sapling.ai/llm/llama?WT.mc_id=academic-105485-koreyst).

Проприетарные модели - это модели, которые принадлежат компании и не предоставляются общественности. Эти модели часто оптимизированы для использования в производстве. Однако их нельзя проверять, изменять или настраивать для различных случаев использования. Кроме того, они не всегда доступны бесплатно, для их использования может потребоваться подписка или оплата. Кроме того, пользователи не имеют контроля над данными, которые используются для обучения модели, а значит, они должны доверить владельцу модели обеспечение конфиденциальности данных и ответственного использования ИИ. Примеры популярных проприетарных моделей включают [OpenAI models](https://platform.openai.com/docs/models/overview?WT.mc_id=academic-105485-koreyst), [Google Bard](https://sapling.ai/llm/bard?WT.mc_id=academic-105485-koreyst) или [Claude 2](https://www.anthropic.com/index/claude-2?WT.mc_id=academic-105485-koreyst).

### Встраивание по сравнению с генерацией изображений по сравнению с генерацией текста и кода

LLM также можно классифицировать по генерируемому ими результату.

Вкрапления - это набор моделей, которые могут преобразовывать текст в числовую форму, называемую вкраплением, которое является числовым представлением входного текста. Модели вкраплений облегчают машинам понимание взаимосвязей между словами или предложениями и могут использоваться в качестве входных данных другими моделями, например моделями классификации или кластеризации, которые лучше работают с числовыми данными. Модели с вкраплениями часто используются для трансферного обучения, когда модель строится для суррогатной задачи, для которой имеется большое количество данных, а затем веса модели (вкрапления) повторно используются для других последующих задач. Примером этой категории является [OpenAI embeddings](https://platform.openai.com/docs/models/embeddings?WT.mc_id=academic-105485-koreyst).

![Embedding](./images/Embedding.png?WT.mc_id=academic-105485-koreyst)

Модели генерации изображений - это модели, которые генерируют изображения. Эти модели часто используются для редактирования изображений, синтеза изображений и перевода изображений. Модели генерации изображений часто обучаются на больших массивах данных изображений, таких как [LAION-5B](https://laion.ai/blog/laion-5b/?WT.mc_id=academic-105485-koreyst), и могут использоваться для создания новых изображений или редактирования существующих с помощью методов инкрустации, суперразрешения и колоризации. Примеры включают [DALL-E-3](https://openai.com/dall-e-3?WT.mc_id=academic-105485-koreyst) и [Stable Diffusion models](https://github.com/Stability-AI/StableDiffusion?WT.mc_id=academic-105485-koreyst).

![Создание изображений](./images/Image.png?WT.mc_id=academic-105485-koreyst)

Модели генерации текста и кода - это модели, которые генерируют текст или код. Такие модели часто используются для обобщения текста, перевода и ответов на вопросы. Модели генерации текста часто обучаются на больших наборах данных текста, таких как [BookCorpus](https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zhu_Aligning_Books_and_ICCV_2015_paper.html?WT.mc_id=academic-105485-koreyst), и могут быть использованы для создания нового текста или для ответов на вопросы. Модели генерации кода, такие как [CodeParrot](https://huggingface.co/codeparrot?WT.mc_id=academic-105485-koreyst), часто обучаются на больших массивах кода, таких как GitHub, и могут использоваться для генерации нового кода или для исправления ошибок в существующем коде.

 ![Text and code generation](./images/Text.png?WT.mc_id=academic-105485-koreyst)

### Кодер-декодер в сравнении с только декодером

Чтобы рассказать о различных типах архитектур LLM, давайте воспользуемся аналогией.

Представьте, что ваш руководитель дал вам задание написать контрольную работу для студентов.  У вас есть два коллеги: один отвечает за создание контента, а другой - за его рецензирование.

Создатель контента - это как модель Decoder only, он может посмотреть на тему и увидеть, что вы уже написали, а затем на основе этого написать курс. Они очень хороши в написании увлекательного и информативного контента, но не очень хороши в понимании темы и целей обучения. Примерами моделей Decoder являются модели семейства GPT, например GPT-3.

Рецензент похож только на модель Encoder, он смотрит на написанный курс и ответы, замечает взаимосвязь между ними и понимает контекст, но не очень хорош в создании контента. Примером модели "только кодировщик" может быть BERT.

Представьте, что у нас есть кто-то, кто может создавать и проверять тесты, - это модель кодировщика-декодировщика. Примерами могут служить BART и T5.

### Сервис в сравнении с моделью

Теперь давайте поговорим о разнице между сервисом и моделью. Сервис - это продукт, предлагаемый поставщиком облачных услуг, который часто представляет собой комбинацию моделей, данных и других компонентов. Модель является основным компонентом сервиса и часто представляет собой базовую модель, например LLM.

Сервисы часто оптимизированы для производственного использования и зачастую проще в использовании, чем модели, благодаря графическому интерфейсу пользователя. Однако сервисы не всегда доступны бесплатно и могут требовать подписки или оплаты за использование, в обмен на использование оборудования и ресурсов владельца сервиса, оптимизацию расходов и легкое масштабирование. Примером такого сервиса является [Azure OpenAI service](https://learn.microsoft.com/azure/ai-services/openai/overview?WT.mc_id=academic-105485-koreyst), который предлагает тарифный план с оплатой по факту использования, то есть с пользователей взимается плата пропорционально количеству использования сервиса. Кроме того, Azure OpenAI service предлагает безопасность корпоративного уровня и ответственную структуру ИИ поверх возможностей моделей.

Модели - это просто нейросеть, с параметрами, весами и прочим. Однако, чтобы позволить компаниям работать локально, необходимо купить оборудование, построить структуру для масштабирования и приобрести лицензию или использовать модель с открытым исходным кодом. Такая модель, как LLaMA, доступна для использования, но для ее запуска требуются вычислительные мощности.

## Как тестировать и повторять различные модели, чтобы понять производительность в Azure

После того как наша команда изучила существующий ландшафт LLM и определила несколько хороших кандидатов для своих сценариев, следующим шагом станет их тестирование на своих данных и в своей рабочей нагрузке. Это итеративный процесс, осуществляемый с помощью экспериментов и измерений.
Большинство моделей, о которых мы упоминали в предыдущих пунктах (модели OpenAI, модели с открытым исходным кодом, такие как Llama2, и трансформаторы Hugging Face), доступны в каталоге [Foundation Models](https://learn.microsoft.com/azure/machine-learning/concept-foundation-models?WT.mc_id=academic-105485-koreyst) в [Azure Machine Learning studio](https://ml.azure.com/?WT.mc_id=academic-105485-koreyst).

[Azure Machine Learning](https://azure.microsoft.com/products/machine-learning/?WT.mc_id=academic-105485-koreyst) - это облачный сервис, предназначенный для специалистов по изучению данных и инженеров ML, чтобы управлять всем жизненным циклом ML (обучение, тестирование, развертывание и обработка MLOps) на единой платформе. Студия Machine Learning предлагает графический пользовательский интерфейс к этой службе и позволяет пользователю:

- Найдите интересующую вас модель Foundation Model в каталоге, отфильтровав ее по задаче, лицензии или названию. Также можно импортировать новые модели, которые еще не включены в каталог.
- Просмотрите карточку модели, включая подробное описание и примеры кода, и протестируйте ее с помощью виджета Sample Inference, предоставив пример подсказки для проверки результата.

![Model card](./images/Llama1.png?WT.mc_id=academic-105485-koreyst)

- Оцените производительность модели с помощью объективных оценочных показателей на конкретной рабочей нагрузке и конкретном наборе данных, предоставленных на вход.

![Model evaluation](./images/Llama2.png?WT.mc_id=academic-105485-koreyst)

- Донастройте модель на пользовательских обучающих данных для повышения производительности модели в конкретной рабочей нагрузке, используя возможности экспериментов и отслеживания в Azure Machine Learning.

![Model fine-tuning](./images/Llama3.png?WT.mc_id=academic-105485-koreyst)

- Разверните исходную предварительно обученную модель или доработанную версию на удаленную конечную точку для выводов в реальном времени или в пакетном режиме, чтобы приложения могли ее использовать.

![Model deployment](./images/Llama4.png?WT.mc_id=academic-105485-koreyst)

## Улучшение результатов обучения по программе LLM

Вместе с командой нашего стартапа мы изучили различные виды LLM и облачную платформу (Azure Machine Learning), позволяющую сравнивать различные модели, оценивать их на тестовых данных, улучшать производительность и внедрять на конечные точки вывода.

Но когда стоит задуматься о тонкой настройке модели, а не об использовании предварительно обученной? Существуют ли другие подходы к повышению производительности моделей на конкретных рабочих нагрузках?

Существует несколько подходов, которые может использовать компания для получения нужных результатов от LLM. Вы можете выбрать различные типы моделей с разной степенью подготовки

развернуть LLM в производстве с различными уровнями сложности, стоимости и качества. Вот несколько различных подходов:

- **Промпт инжиниринг с использованием контекста**. Идея заключается в том, чтобы предоставить достаточно контекста, когда вы запрашиваете, чтобы получить нужные ответы.

- **Генерация с поддержкой извлечения данных, RAG**. Например, ваши данные могут находиться в базе данных или в конечной точке веб-сайта. Чтобы обеспечить включение этих данных или их подмножества во время запроса, вы можете получить соответствующие данные и сделать их частью запроса пользователя.

- **Модель с тонкой настройкой**. В этом случае вы обучаете модель на собственных данных, что приводит к тому, что модель становится более точной и отвечает вашим потребностям, но может быть дорогостоящей.

![LLMs deployment](./images/Deploy.png?WT.mc_id=academic-105485-koreyst)

Img source: [Four Ways that Enterprises Deploy LLMs | Fiddler AI Blog](https://www.fiddler.ai/blog/four-ways-that-enterprises-deploy-llms?WT.mc_id=academic-105485-koreyst)

### Prompt Engineering with Context

Предварительно обученные LLM очень хорошо справляются с обобщенными задачами на естественном языке, даже если вызывать их с помощью короткой подсказки, например, предложения для завершения или вопроса - так называемое обучение "с нуля".

Однако чем больше пользователь может сформулировать свой запрос с подробной просьбой и примерами - Контекст - тем точнее и ближе к ожиданиям пользователя будет ответ. В этом случае мы говорим об "одноразовом" обучении, если подсказка содержит только один пример, и о "многоразовом обучении", если она включает несколько примеров.
Разработка подсказок с учетом контекста - наиболее экономически эффективный подход, с которого стоит начать.

### Retrieval Augmented Generation (RAG)

У LLM есть ограничение: для получения ответа они могут использовать только те данные, которые были использованы во время их обучения. Это означает, что они ничего не знают о фактах, произошедших после процесса обучения, и не могут получить доступ к непубличной информации (например, данным компании).
Эту проблему можно решить с помощью RAG - техники, которая дополняет подсказки внешними данными в виде фрагментов документов, учитывая ограничения на длину подсказки. Это поддерживается инструментами векторной базы данных (например, [Azure Vector Search](https://learn.microsoft.com/azure/search/vector-search-overview?WT.mc_id=academic-105485-koreyst)), которые извлекают полезные фрагменты из различных предварительно определенных источников данных и добавляют их в контекст подсказки.

Эта техника очень полезна, когда у предприятия нет достаточного количества данных, времени или ресурсов для точной настройки LLM, но при этом оно хочет повысить производительность на конкретной рабочей нагрузке и снизить риски фабрикации, то есть искажения реальности или вредного контента.  

### Fine-tuned model

Тонкая настройка - это процесс, использующий трансферное обучение для "адаптации" модели к последующей задаче или решению конкретной проблемы. В отличие от обучения с несколькими выстрелами и RAG, в результате создается новая модель с обновленными весами и смещениями. Для этого требуется набор обучающих примеров, состоящий из одного входа (подсказки) и связанного с ним выхода (завершения).
Этот подход будет предпочтительным, если:

- **Using fine-tuned models**. Бизнесмены предпочитают использовать не высокопроизводительные модели (например, модели встраивания), а тонко настроенные менее производительные модели, что позволяет получить более экономичное и быстрое решение.

- **Considering latency**. Latency is important for a specific use-case, so it’s not possible to use very long prompts or the number of examples that should be learned from the model doesn’t fit with the prompt length limit.

- **Staying up to date**. A business has a lot of high-quality data and ground truth labels and the resources required to maintain this data up to date over time.

### Trained model

Подготовка LLM с нуля - это, несомненно, самый сложный подход, требующий огромного количества данных, квалифицированных специалистов и соответствующих вычислительных мощностей. Этот вариант следует рассматривать только в том случае, если у компании есть специфический для данной области сценарий использования и большой объем данных, ориентированных на эту область.

## Knowledge check

Что может быть хорошим подходом для улучшения результатов окончания LLM?

1. Оперативное проектирование с использованием контекста
1. RAG
1. Тонкая настройка модели

A:3, если у вас есть время, ресурсы и высококачественные данные, то тонкая настройка - лучший вариант, чтобы оставаться в курсе событий. Однако если вы хотите улучшить ситуацию, но у вас не хватает времени, стоит сначала рассмотреть RAG.

## 🚀 Вызов

Узнайте больше о том, как вы можете [использовать RAG](https://learn.microsoft.com/azure/search/retrieval-augmented-generation-overview?WT.mc_id=academic-105485-koreyst) для своего бизнеса.

## Отличная работа, продолжайте учиться

After completing this lesson, check out our [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) to continue leveling up your Generative AI knowledge!

Head over to Lesson 3 where we will look at how to [build with Generative AI Responsibly](../03-using-generative-ai-responsibly/README.md?WT.mc_id=academic-105485-koreyst)!
